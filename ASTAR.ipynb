{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993690c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import heapq\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a924d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterZoneEnv:\n",
    "    \"\"\"\n",
    "    A 2D grid environment for a drone exploring a disaster zone.\n",
    "\n",
    "    Legend (internally stored as integers):\n",
    "        0 -> Empty cell\n",
    "        1 -> Obstacle\n",
    "        2 -> Survivor\n",
    "        3 -> Recharging Point\n",
    "\n",
    "    'D' in a scenario array indicates the drone's starting position (only in predefined scenarios).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 width=8, \n",
    "                 height=8, \n",
    "                 num_obstacles=5, \n",
    "                 num_survivors=3, \n",
    "                 num_resources=2, \n",
    "                 initial_energy=20, \n",
    "                 dynamic=False, \n",
    "                 predefined_grid=None):\n",
    "        \"\"\"\n",
    "        Initialize the environment. If a predefined grid is provided, use it; otherwise, generate randomly.\n",
    "\n",
    "        :param width: Width of the grid.\n",
    "        :param height: Height of the grid.\n",
    "        :param num_obstacles: Number of obstacles.\n",
    "        :param num_survivors: Number of survivors.\n",
    "        :param num_resources: Number of recharging points.\n",
    "        :param initial_energy: Initial energy of the drone.\n",
    "        :param dynamic: Whether the environment is dynamic.\n",
    "        :param predefined_grid: A predefined grid layout (optional, can contain 'D').\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.num_obstacles = num_obstacles\n",
    "        self.num_survivors = num_survivors\n",
    "        self.num_resources = num_resources\n",
    "        self.initial_energy = initial_energy\n",
    "        self.dynamic = dynamic\n",
    "        self.energy = initial_energy\n",
    "        self.dynamic_changes = 0  # Counter for dynamic changes\n",
    "\n",
    "        # If predefined_grid is given, use that scenario\n",
    "        if predefined_grid is not None:\n",
    "            self.reset_with_scenario(predefined_grid)\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "    def _get_random_empty_cell(self):\n",
    "        \"\"\"\n",
    "        Finds a random empty cell in the grid (cell == 0).\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            x = random.randint(0, self.height - 1)\n",
    "            y = random.randint(0, self.width - 1)\n",
    "            if self.grid[x, y] == 0:  # Ensure the cell is empty\n",
    "                return x, y\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to a new random configuration.\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
    "\n",
    "        # Place obstacles\n",
    "        for _ in range(self.num_obstacles):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 1\n",
    "\n",
    "        # Place survivors\n",
    "        for _ in range(self.num_survivors):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 2\n",
    "\n",
    "        # Place resources\n",
    "        for _ in range(self.num_resources):\n",
    "            x, y = self._get_random_empty_cell()\n",
    "            self.grid[x, y] = 3\n",
    "\n",
    "        # Random start for the drone\n",
    "        self.drone_x, self.drone_y = self._get_random_empty_cell()\n",
    "        self.energy = self.initial_energy\n",
    "\n",
    "    def reset_with_scenario(self, scenario):\n",
    "        \"\"\"\n",
    "        Loads a predefined grid (possibly containing 'D') into the environment.\n",
    "        We convert that scenario into an integer grid internally.\n",
    "        \"\"\"\n",
    "        # Ensure scenario is at least a NumPy array\n",
    "        scenario = np.array(scenario, dtype=object)  # Force an object array (handles mixed int/'D')\n",
    "\n",
    "        # Prepare an integer grid of the same shape\n",
    "        self.height, self.width = scenario.shape\n",
    "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
    "\n",
    "        drone_positions = []\n",
    "\n",
    "        # Convert each cell from scenario into our integer grid\n",
    "        for x in range(self.height):\n",
    "            for y in range(self.width):\n",
    "                cell_value = scenario[x, y]\n",
    "                if cell_value == 'D':\n",
    "                    # Mark drone position\n",
    "                    drone_positions.append((x, y))\n",
    "                    self.grid[x, y] = 0  # The drone is effectively on an empty cell\n",
    "                else:\n",
    "                    # Convert string or int to int properly\n",
    "                    # If cell_value is already an int, this will do nothing special.\n",
    "                    # If it's a string like '1', '2', '3', or '0', convert to int.\n",
    "                    self.grid[x, y] = int(cell_value)\n",
    "\n",
    "        if len(drone_positions) != 1:\n",
    "            raise ValueError(\"Scenario must contain exactly one 'D' for the drone's starting position.\")\n",
    "\n",
    "        self.drone_x, self.drone_y = drone_positions[0]\n",
    "        self.energy = self.initial_energy\n",
    "\n",
    "    def apply_dynamic_changes(self, step_count):\n",
    "        \"\"\"\n",
    "        Applies dynamic changes to the environment if self.dynamic is True.\n",
    "\n",
    "        :param step_count: Current step count of the simulation.\n",
    "        \"\"\"\n",
    "        if self.dynamic:\n",
    "            # Example dynamic behavior:\n",
    "            # 1) Add a new obstacle every 5 steps\n",
    "            if step_count % 5 == 0:\n",
    "                x, y = self._get_random_empty_cell()\n",
    "                self.grid[x, y] = 1\n",
    "                self.dynamic_changes += 1\n",
    "\n",
    "            # 2) Move all survivors every 3 steps\n",
    "            if step_count % 3 == 0:\n",
    "                survivor_positions = [(x, y) for x in range(self.height) \n",
    "                                      for y in range(self.width) if self.grid[x, y] == 2]\n",
    "                for (sx, sy) in survivor_positions:\n",
    "                    self.grid[sx, sy] = 0  # Remove old survivor\n",
    "                    new_x, new_y = self._get_random_empty_cell()\n",
    "                    self.grid[new_x, new_y] = 2\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Renders the grid for visualization in a text-based manner.\n",
    "        \"\"\"\n",
    "        grid_copy = self.grid.astype(str)\n",
    "        grid_copy[grid_copy == '0'] = '.'\n",
    "        grid_copy[grid_copy == '1'] = '#'\n",
    "        grid_copy[grid_copy == '2'] = 'S'\n",
    "        grid_copy[grid_copy == '3'] = 'R'\n",
    "\n",
    "        # Mark the drone in the visualization\n",
    "        grid_copy[self.drone_x, self.drone_y] = 'D'\n",
    "\n",
    "        for row in grid_copy:\n",
    "            print(\" \".join(row))\n",
    "        print(f\"Energy: {self.energy}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad2704d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class AStarDrone:\n",
    "    def __init__(self, env, initial_position, goal_position):\n",
    "        # Initialize the drone with environment, starting, and goal positions\n",
    "        self.env = env\n",
    "        self.position = initial_position\n",
    "        self.goal = goal_position\n",
    "        self.visited = set()  # Track visited positions\n",
    "        self.path = []  # Track the full path of positions\n",
    "        self.energy = env.initial_energy  # Initial energy from the environment\n",
    "        self.steps_taken = 0  # Track number of steps taken\n",
    "        self.survivors_rescued = 0  # Track number of survivors rescued\n",
    "        self.resources_collected = 0  # Track number of resources collected\n",
    "        self.energy_used = 0  # Track total energy used\n",
    "\n",
    "    def move(self):\n",
    "        # Stop if drone reaches its goal\n",
    "        if self.position == self.goal:\n",
    "            print(f\"Goal reached at {self.position}\")\n",
    "            return True\n",
    "\n",
    "        # Stop if drone runs out of energy\n",
    "        if self.energy <= 0:\n",
    "            print(\"Out of energy!\")\n",
    "            return False\n",
    "\n",
    "        # A* Search to find the next best move\n",
    "        next_move = self.a_star_search(self.position, self.goal)\n",
    "\n",
    "        if next_move:\n",
    "            # Calculate the action to take and update position\n",
    "            action = self.calculate_action(self.position, next_move)\n",
    "\n",
    "            # Update the drone's position and mark it as visited\n",
    "            self.position = next_move\n",
    "            self.visited.add(self.position)\n",
    "            self.path.append(self.position)\n",
    "\n",
    "            # Deduct energy for each move and track energy usage\n",
    "            self.energy -= 1\n",
    "            self.energy_used += 1\n",
    "            self.steps_taken += 1\n",
    "\n",
    "            # Apply dynamic changes to the environment\n",
    "            self.env.apply_dynamic_changes(self.steps_taken)\n",
    "\n",
    "            # Print grid and current position\n",
    "            print(f\"Current Grid:\")\n",
    "            for row in self.env.grid:\n",
    "                print(row)\n",
    "\n",
    "            print(f\"Moved to {self.position}, Current Cell: {self.env.grid[self.position[0]][self.position[1]]}\")\n",
    "            print(f\"Energy left: {self.energy}\")\n",
    "\n",
    "            # Check grid value for survivors or resources\n",
    "            current_cell = self.env.grid[self.position[0]][self.position[1]]\n",
    "            print(f\"Current grid value: {current_cell}\")\n",
    "\n",
    "            if current_cell == 2:  # '2' for survivors\n",
    "                self.survivors_rescued += 1\n",
    "                self.env.grid[self.position[0]][self.position[1]] = 0  # Mark survivor as rescued\n",
    "                print(f\"Survivor rescued at {self.position}\")\n",
    "\n",
    "            if current_cell == 3:  # '3' for resources\n",
    "                self.resources_collected += 1\n",
    "                self.env.grid[self.position[0]][self.position[1]] = 0  # Mark resource as collected\n",
    "                print(f\"Resource collected at {self.position}\")\n",
    "\n",
    "                # Make sure energy does not exceed the initial energy limit\n",
    "                self.energy = min(self.energy + 5, self.env.initial_energy)\n",
    "                print(f\"Energy replenished! Current energy: {self.energy}\")\n",
    "\n",
    "            return False\n",
    "        else:\n",
    "            print(\"No valid move found.\")\n",
    "            return False\n",
    "\n",
    "    def a_star_search(self, start, goal):\n",
    "        \"\"\"Perform A* search to find the best path from start to goal.\"\"\"\n",
    "        open_list = []\n",
    "        heapq.heappush(open_list, (0, start))  # Priority queue (min-heap) based on f-score\n",
    "        came_from = {}\n",
    "        g_score = {start: 0}\n",
    "        f_score = {start: self.heuristic(start, goal)}\n",
    "        visited = set()\n",
    "\n",
    "        while open_list:\n",
    "            _, current = heapq.heappop(open_list)  # Get node with the lowest f-score\n",
    "            visited.add(current)\n",
    "\n",
    "            # If the goal is reached, reconstruct the path\n",
    "            if current == goal:\n",
    "                return self.reconstruct_path(came_from, current)\n",
    "\n",
    "            for neighbor in self.get_neighbors(current):\n",
    "                if neighbor in visited or self.env.grid[neighbor[0], neighbor[1]] == 1:\n",
    "                    continue  # Skip visited or invalid cells\n",
    "\n",
    "                tentative_g_score = g_score[current] + 1  # Assume uniform cost for movement\n",
    "\n",
    "                # Update scores if a better path is found\n",
    "                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n",
    "                    came_from[neighbor] = current\n",
    "                    g_score[neighbor] = tentative_g_score\n",
    "                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal)\n",
    "\n",
    "                    # Add neighbor to the priority queue\n",
    "                    heapq.heappush(open_list, (f_score[neighbor], neighbor))\n",
    "\n",
    "        return None  # No valid path found\n",
    "\n",
    "    def get_neighbors(self, position):\n",
    "        \"\"\"Get valid neighboring cells that are not obstacles.\"\"\"\n",
    "        x, y = position\n",
    "        neighbors = []\n",
    "\n",
    "        # Define directions for up, down, left, right\n",
    "        directions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # (dx, dy)\n",
    "        for dx, dy in directions:\n",
    "            nx, ny = x + dx, y + dy\n",
    "\n",
    "            # Ensure the position is within bounds and not an obstacle\n",
    "            if 0 <= nx < self.env.height and 0 <= ny < self.env.width and self.env.grid[nx][ny] != 1:\n",
    "                neighbors.append((nx, ny))\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def heuristic(self, position, goal):\n",
    "        \"\"\"Manhattan distance as heuristic for A* search.\"\"\"\n",
    "        return abs(position[0] - goal[0]) + abs(position[1] - goal[1])\n",
    "\n",
    "    def reconstruct_path(self, came_from, current):\n",
    "        \"\"\"Reconstruct the path from start to goal using the came_from map.\"\"\"\n",
    "        path = []\n",
    "        while current in came_from:\n",
    "            path.append(current)\n",
    "            current = came_from[current]\n",
    "        path.reverse()  # Reverse to get the correct order\n",
    "        return path[0] if path else None  # Return the first position in the path\n",
    "\n",
    "    def calculate_action(self, current, next_position):\n",
    "        \"\"\"Calculate the action based on the direction of movement.\"\"\"\n",
    "        dx, dy = next_position[0] - current[0], next_position[1] - current[1]\n",
    "\n",
    "        # Define actions based on movement direction\n",
    "        if dx == -1 and dy == 0:\n",
    "            return 'UP'\n",
    "        elif dx == 1 and dy == 0:\n",
    "            return 'DOWN'\n",
    "        elif dx == 0 and dy == -1:\n",
    "            return 'LEFT'\n",
    "        elif dx == 0 and dy == 1:\n",
    "            return 'RIGHT'\n",
    "        else:\n",
    "            raise ValueError(\"Invalid move: Action not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e98786f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class ScenarioTester:\n",
    "\n",
    "    def __init__(self, agent_class, env_params):\n",
    "        \"\"\"\n",
    "        Initialize the tester with the agent class and environment parameters.\n",
    "\n",
    "        :param agent_class: The agent class to be tested (e.g., AStarDrone).\n",
    "        :param env_params: Dictionary of environment parameters.\n",
    "        \"\"\"\n",
    "        self.agent_class = agent_class\n",
    "        self.env_params = env_params\n",
    "        self.results = []  # To store results of each scenario\n",
    "\n",
    "    def get_initial_and_goal_positions(self, grid):\n",
    "        \"\"\"\n",
    "        Extracts the initial position (marked 'D') and a goal position (e.g., survivor or resource).\n",
    "        \"\"\"\n",
    "        initial_position = None\n",
    "        goal_position = None\n",
    "        for x in range(len(grid)):\n",
    "            for y in range(len(grid[0])):\n",
    "                if grid[x][y] == 0:  # Assuming 0 is the empty cell and 'D' corresponds to drone\n",
    "                    initial_position = (x, y)\n",
    "                elif grid[x][y] == 2:  # 2 represents survivor\n",
    "                    goal_position = (x, y)\n",
    "\n",
    "        # If no goal is found, set it to a default position\n",
    "        if not goal_position:\n",
    "            goal_position = (len(grid) - 1, len(grid[0]) - 1)  # Default to bottom-right corner\n",
    "        return initial_position, goal_position\n",
    "      \n",
    "    def run_scenario(self, scenario, scenario_id):\n",
    "        \"\"\"\n",
    "        Run a predefined scenario with initial setup and agent's execution.\n",
    "    \n",
    "        :param scenario: A predefined grid for the environment.\n",
    "        :param scenario_id: Identifier for the scenario.\n",
    "        \"\"\"\n",
    "        # Get initial and goal positions from the scenario grid\n",
    "        initial_position, goal_position = self.get_initial_and_goal_positions(scenario)\n",
    "\n",
    "        # Create environment with the specified scenario\n",
    "        env = DisasterZoneEnv(\n",
    "            predefined_grid=scenario,\n",
    "            initial_energy=self.env_params.get(\"initial_energy\", 20),\n",
    "            dynamic=self.env_params.get(\"dynamic\", False)\n",
    "        )\n",
    "\n",
    "        # Initialize the agent with the initial and goal positions\n",
    "        agent = self.agent_class(env, initial_position, goal_position)\n",
    "\n",
    "        # Measure execution time\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Starting scenario {scenario_id} with initial position: {initial_position} and goal position: {goal_position}\")\n",
    "    \n",
    "        step_count = 0\n",
    "        while not agent.move():  # Calling move() continuously\n",
    "            # Apply dynamic changes if necessary\n",
    "            env.apply_dynamic_changes(step_count)\n",
    "        \n",
    "            # Optionally, render the environment for visualization\n",
    "            env.render()\n",
    "\n",
    "            # Track energy and position\n",
    "            print(f\"Agent at position {agent.position}, Energy: {env.energy}\")\n",
    "            print(f\"Current grid value: {env.grid[agent.position[0]][agent.position[1]]}\")\n",
    "        \n",
    "            step_count += 1\n",
    "\n",
    "        end_time = time.time()\n",
    "        computation_time = end_time - start_time\n",
    "\n",
    "        # Record results\n",
    "        self.results.append({\n",
    "            \"Agent Name\": self.agent_class.__name__,\n",
    "            \"Scenario ID\": scenario_id,\n",
    "            \"Steps Taken\": getattr(agent, \"steps_taken\", \"N/A\"),\n",
    "            \"Survivors Rescued\": getattr(agent, \"survivors_rescued\", \"N/A\"),\n",
    "            \"Resources Collected\": getattr(agent, \"resources_collected\", \"N/A\"),\n",
    "            \"Energy Used\": self.env_params.get(\"initial_energy\", 20) - env.energy,\n",
    "            \"Computation Time (s)\": computation_time\n",
    "        })\n",
    "\n",
    "        print(f\"Results for {scenario_id}:\")\n",
    "        print(f\"Survivors Rescued: {agent.survivors_rescued}\")\n",
    "        print(f\"Resources Collected: {agent.resources_collected}\")\n",
    "        print(f\"Energy Used: {self.env_params.get('initial_energy', 20) - env.energy}\")\n",
    "        print(f\"Computation Time (s): {computation_time:.6f}\")\n",
    "        \n",
    "    def display_results(self):\n",
    "        \"\"\"\n",
    "        Display results for all tested scenarios in a formatted table.\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No scenarios have been tested yet.\")\n",
    "            return\n",
    "\n",
    "        # Display results in a tabular format\n",
    "        print(\"=\" * 100)\n",
    "        print(f\"{'Agent Name':<15} {'Scenario ID':<15} {'Steps Taken':<15} {'Survivors Rescued':<20} {'Resources Collected':<20} {'Energy Used':<15} {'Computation Time (s)':<20}\")\n",
    "        print(\"=\" * 100)\n",
    "\n",
    "        for result in self.results:\n",
    "            print(f\"{result['Agent Name']:<15} {result['Scenario ID']:<15} {result['Steps Taken']:<15} {result['Survivors Rescued']:<20} {result['Resources Collected']:<20} {result['Energy Used']:<15} {result['Computation Time (s)']:<20.6f}\")\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aae494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scenario Scenario 1 with initial position: (4, 4) and goal position: (1, 3)\n",
      "Current Grid:\n",
      "[0 0 0 0 1]\n",
      "[0 1 0 2 0]\n",
      "[0 0 3 0 0]\n",
      "[1 0 0 0 0]\n",
      "[0 0 0 1 0]\n",
      "Moved to (3, 4), Current Cell: 0\n",
      "Energy left: 19\n",
      "Current grid value: 0\n",
      "D . . . #\n",
      ". # . . .\n",
      ". . R . .\n",
      "# . # . .\n",
      ". . . # S\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (3, 4), Energy: 20\n",
      "Current grid value: 0\n",
      "Current Grid:\n",
      "[0 0 0 0 1]\n",
      "[0 1 0 0 0]\n",
      "[0 0 3 0 0]\n",
      "[1 0 1 0 0]\n",
      "[0 0 0 1 2]\n",
      "Moved to (2, 4), Current Cell: 0\n",
      "Energy left: 18\n",
      "Current grid value: 0\n",
      "D . . . #\n",
      ". # . . .\n",
      ". . R . .\n",
      "# . # . .\n",
      ". . . # S\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (2, 4), Energy: 20\n",
      "Current grid value: 0\n",
      "Current Grid:\n",
      "[0 0 0 2 1]\n",
      "[0 1 0 0 0]\n",
      "[0 0 3 0 0]\n",
      "[1 0 1 0 0]\n",
      "[0 0 0 1 0]\n",
      "Moved to (1, 4), Current Cell: 0\n",
      "Energy left: 17\n",
      "Current grid value: 0\n",
      "D . . S #\n",
      ". # . . .\n",
      ". . R . .\n",
      "# . # . .\n",
      ". . . # .\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (1, 4), Energy: 20\n",
      "Current grid value: 0\n",
      "Current Grid:\n",
      "[0 0 0 2 1]\n",
      "[0 1 0 0 0]\n",
      "[0 0 3 0 0]\n",
      "[1 0 1 0 0]\n",
      "[0 0 0 1 0]\n",
      "Moved to (1, 3), Current Cell: 0\n",
      "Energy left: 16\n",
      "Current grid value: 0\n",
      "D . . . #\n",
      ". # . . .\n",
      ". . R . S\n",
      "# . # . .\n",
      ". . . # .\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (1, 3), Energy: 20\n",
      "Current grid value: 0\n",
      "Goal reached at (1, 3)\n",
      "Results for Scenario 1:\n",
      "Survivors Rescued: 0\n",
      "Resources Collected: 0\n",
      "Energy Used: 0\n",
      "Computation Time (s): 1.613461\n",
      "Starting scenario Scenario 2 with initial position: (3, 3) and goal position: (0, 3)\n",
      "Current Grid:\n",
      "[0 0 1 2]\n",
      "[0 1 0 0]\n",
      "[3 0 1 0]\n",
      "[0 0 0 0]\n",
      "Moved to (2, 3), Current Cell: 0\n",
      "Energy left: 19\n",
      "Current grid value: 0\n",
      "D . # .\n",
      ". # # .\n",
      "R . # .\n",
      ". . . S\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (2, 3), Energy: 20\n",
      "Current grid value: 0\n",
      "Current Grid:\n",
      "[0 0 1 0]\n",
      "[0 1 1 0]\n",
      "[3 0 1 0]\n",
      "[0 0 0 2]\n",
      "Moved to (1, 3), Current Cell: 0\n",
      "Energy left: 18\n",
      "Current grid value: 0\n",
      "D . # .\n",
      ". # # .\n",
      "R . # .\n",
      ". . . S\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (1, 3), Energy: 20\n",
      "Current grid value: 0\n",
      "Current Grid:\n",
      "[2 0 1 0]\n",
      "[0 1 1 0]\n",
      "[3 0 1 0]\n",
      "[0 0 0 0]\n",
      "Moved to (0, 3), Current Cell: 0\n",
      "Energy left: 17\n",
      "Current grid value: 0\n",
      "D . # .\n",
      ". # # .\n",
      "R . # .\n",
      ". . . .\n",
      "Energy: 20\n",
      "\n",
      "Agent at position (0, 3), Energy: 20\n",
      "Current grid value: 0\n",
      "Goal reached at (0, 3)\n",
      "Results for Scenario 2:\n",
      "Survivors Rescued: 0\n",
      "Resources Collected: 0\n",
      "Energy Used: 0\n",
      "Computation Time (s): 0.001012\n",
      "====================================================================================================\n",
      "Agent Name      Scenario ID     Steps Taken     Survivors Rescued    Resources Collected  Energy Used     Computation Time (s)\n",
      "====================================================================================================\n",
      "AStarDrone      Scenario 1      4               0                    0                    0               1.613461            \n",
      "AStarDrone      Scenario 2      3               0                    0                    0               0.001012            \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example agent class (replace with your actual agent class)\n",
    "    agent_class = AStarDrone  # Replace with your actual agent class\n",
    "    \n",
    "    # Example environment parameters (you can adjust as needed)\n",
    "    env_params = {\n",
    "        \"initial_energy\": 20,\n",
    "        \"dynamic\": True  # Set dynamic to True for dynamic environment behavior\n",
    "    }\n",
    "    \n",
    "    # Define different scenarios (these grids can be random or predefined)\n",
    "    scenario_1 = [\n",
    "        ['D', 0, 0, 0, 1],\n",
    "        [0, 1, 0, 2, 0],\n",
    "        [0, 0, 3, 0, 0],\n",
    "        [1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0]\n",
    "    ]\n",
    "\n",
    "    scenario_2 = [\n",
    "        ['D', 0, 1, 2],\n",
    "        [0, 1, 0, 0],\n",
    "        [3, 0, 1, 0],\n",
    "        [0, 0, 0, 0]\n",
    "    ]\n",
    "\n",
    "    # Initialize the ScenarioTester\n",
    "    tester = ScenarioTester(agent_class, env_params)\n",
    "\n",
    "    # Run different scenarios with identifiers\n",
    "    tester.run_scenario(scenario_1, \"Scenario 1\")\n",
    "    tester.run_scenario(scenario_2, \"Scenario 2\")\n",
    "\n",
    "    # Display results\n",
    "    tester.display_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
